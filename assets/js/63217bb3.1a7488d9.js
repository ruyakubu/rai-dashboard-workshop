"use strict";(self.webpackChunkrai_workshop=self.webpackChunkrai_workshop||[]).push([[759],{3905:(e,t,a)=>{a.d(t,{Zo:()=>c,kt:()=>h});var i=a(7294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function o(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);t&&(i=i.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,i)}return a}function n(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?o(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function l(e,t){if(null==e)return{};var a,i,r=function(e,t){if(null==e)return{};var a,i,r={},o=Object.keys(e);for(i=0;i<o.length;i++)a=o[i],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(i=0;i<o.length;i++)a=o[i],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var s=i.createContext({}),d=function(e){var t=i.useContext(s),a=t;return e&&(a="function"==typeof e?e(t):n(n({},t),e)),a},c=function(e){var t=d(e.components);return i.createElement(s.Provider,{value:t},e.children)},p="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return i.createElement(i.Fragment,{},t)}},m=i.forwardRef((function(e,t){var a=e.components,r=e.mdxType,o=e.originalType,s=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),p=d(a),m=r,h=p["".concat(s,".").concat(m)]||p[m]||u[m]||o;return a?i.createElement(h,n(n({ref:t},c),{},{components:a})):i.createElement(h,n({ref:t},c))}));function h(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=a.length,n=new Array(o);n[0]=m;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[p]="string"==typeof e?e:r,n[1]=l;for(var d=2;d<o;d++)n[d]=a[d];return i.createElement.apply(null,n)}return i.createElement.apply(null,a)}m.displayName="MDXCreateElement"},6877:(e,t,a)=>{a.r(t),a.d(t,{contentTitle:()=>n,default:()=>p,frontMatter:()=>o,metadata:()=>l,toc:()=>s});var i=a(7462),r=(a(7294),a(3905));const o={title:"Local Feature Importance",sidebar_position:1,slug:"/view-individual-feature-importance"},n=void 0,l={unversionedId:"lab4-data-analysis copy/view-individual-feature-importance",id:"lab4-data-analysis copy/view-individual-feature-importance",isDocsHomePage:!1,title:"Local Feature Importance",description:"Individual feature importance:  Table view",source:"@site/docs/lab4-data-analysis copy/view-individual-feature-importance.md",sourceDirName:"lab4-data-analysis copy",slug:"/view-individual-feature-importance",permalink:"/rai-dashboard-workshop/docs/view-individual-feature-importance",editUrl:"https://github.com/facebook/docusaurus/edit/main/website/docs/lab4-data-analysis copy/view-individual-feature-importance.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{title:"Local Feature Importance",sidebar_position:1,slug:"/view-individual-feature-importance"},sidebar:"tutorialSidebar",previous:{title:"Global Feature Importance",permalink:"/rai-dashboard-workshop/docs/view-feature-importance"}},s=[{value:"Individual feature importance:  Table view",id:"individual-feature-importance--table-view",children:[]},{value:"Individual conditional expectation (using ICE plot)",id:"individual-conditional-expectation-using-ice-plot",children:[]}],d={toc:s},c="wrapper";function p(e){let{components:t,...o}=e;return(0,r.kt)(c,(0,i.Z)({},d,o,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h2",{id:"individual-feature-importance--table-view"},"Individual feature importance:  Table view"),(0,r.kt)("p",null,"The RAI dashboard Feature Importance section has a table view that enables users to see which records the model made a correct vs incorrect prediction.  You can use each individual patient's record to see which features positively or negatively drove that individual outcome. This is especially useful when debugging to see where the model is performing erroneously for a specific patient, and which features are positive or negative contributors.   In our case, you'll focus on the cohort with the highest errors."),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Individual Feature importance table",src:a(8979).Z,title:"Individual Feature importance table"})),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},'Click on the "switch cohort" link on top of the dashboard.  The select the ',(0,r.kt)("strong",{parentName:"li"},(0,r.kt)("em",{parentName:"strong"},'"Err: Prior_Inpatient >0; Num_meds >11.50 & <= 21.50"')),' cohort under the "Cohort list" drop-down menu list.')),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Switch cohort",src:a(4873).Z,title:"Switch cohort"}),"\t"),(0,r.kt)("ol",{start:2},(0,r.kt)("li",{parentName:"ol"},"Under the ",(0,r.kt)("strong",{parentName:"li"},'"Incorrect predictions"')," table view, select record index #882.  This will generate a ",(0,r.kt)("strong",{parentName:"li"},"Feature Important plot")," chart under the Table view.")),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Individual Feature importance table",src:a(1355).Z,title:"Individual Feature importance table"}),"\t"),(0,r.kt)("ol",{start:3},(0,r.kt)("li",{parentName:"ol"},"Here you will see that ",(0,r.kt)("em",{parentName:"li"},'"Prior_Inpatient"'),", ",(0,r.kt)("em",{parentName:"li"},'"Age"'),", ",(0,r.kt)("em",{parentName:"li"},'"Max_Glu_Serum"')," and ",(0,r.kt)("em",{parentName:"li"},'"num_medications"')," are the top 4 features that are negative contributors to driving our model incorrectly predicting that the selected patient will not be readmitted within 30 days (outcome should be Readmitted)."),(0,r.kt)("li",{parentName:"ol"},"Confirm that the 4 features from record index #882 are different from the top 4 features we saw above driving the models\u2019 overall predictions from the ",(0,r.kt)("strong",{parentName:"li"},"Aggregate feature importance")," tab."),(0,r.kt)("li",{parentName:"ol"},"Next, the Individual feature importance graph shows that the ",(0,r.kt)("em",{parentName:"li"},'"Admission_source"'),", ",(0,r.kt)("em",{parentName:"li"},'"prior_emergency"'),", ",(0,r.kt)("em",{parentName:"li"},'"gender"')," and ",(0,r.kt)("em",{parentName:"li"},'"insulin"')," positively contributed to the model's outcome (Not Readmitted). Since the model incorrectly predicted record index #882 as Not Readmitted, that means the positively contributing features are erroneous: ",(0,r.kt)("em",{parentName:"li"},'"Admission_source"'),", ",(0,r.kt)("em",{parentName:"li"},'"prior_emergency"'),", ",(0,r.kt)("em",{parentName:"li"},'"gender"')," and ",(0,r.kt)("em",{parentName:"li"},'"insulin"')," since they played a significant role in skewing the model's output for this data point."),(0,r.kt)("li",{parentName:"ol"},"Now select record index #865, another record where the model predicted the opposite outcome incorrectly, that a patient would be Readmitted back to a hospital. ")),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Compare datapoints prediction contributions",src:a(9149).Z,title:"Compare datapoint prediction contribution"}),"\t\t"),(0,r.kt)("ol",{start:7},(0,r.kt)("li",{parentName:"ol"},"Confirm that the key feature positively contributing to that model's prediction is ",(0,r.kt)("em",{parentName:"li"},"\u201cPrior_Emergency\u201d"),", and ",(0,r.kt)("em",{parentName:"li"},"\u201cInsulin\u201d"),". "),(0,r.kt)("li",{parentName:"ol"},"Once again, you'll see the top important features (in blue color) that drove the model\u2019s prediction has changed. In this case, ",(0,r.kt)("em",{parentName:"li"},"\u201cPrior_Emergency\u201d")," was the top positive contributor. That means it had major impact in the model\u2019s incorrect prediction in our selected cohort. In trying to debug why a model\u2019s prediction is erroneous for a given data point, this chart provides ML professionals an explanation of which features positively influenced the poor outcome.")),(0,r.kt)("h2",{id:"individual-conditional-expectation-using-ice-plot"},"Individual conditional expectation (using ICE plot)"),(0,r.kt)("p",null,'The RAI dashboard gives you have the ability to select a feature and see the model\'s prediction for the different values in that feature using the Individual Conditional Expectation (ICE) plot. We use the same patients above:  record index #882 and #865.  We using the "A1CResult" feature to see the models prediction for its different values.'),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},'Select the "Individual Conditional Expectation (ICE) plot" radio button. '),(0,r.kt)("li",{parentName:"ol"},"Next, in the Feature drop-down menu, we'll select ",(0,r.kt)("em",{parentName:"li"},'"A1CResult"'),". The orange dots represent record index #882 that the model incorrectly predicted as Not Readmitted. The blue dots represent record index #865 that the model incorrectly predicted as Readmitted.")),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"ICE feature influence",src:a(9149).Z,title:"ICE feature influence"}),"\t"),(0,r.kt)("ol",{start:3},(0,r.kt)("li",{parentName:"ol"},"Under The ICE plot, you'll see the model's predictions for the different ",(0,r.kt)("em",{parentName:"li"},'"A1CResult"'),' values: "Norm", ">7", ">8" and "None" (the "None" when no A1C test). '),(0,r.kt)("li",{parentName:"ol"},"As you can see the chart, a patient has the lowest chance of not being readmitted when the ",(0,r.kt)("em",{parentName:"li"},'"AICResult"'),' result is ">8" for record index #882 and #865. Both the orange and blue dots are showing the lowest point in their cluster at the ">8" mark on the axis, and the model\'s predicted probability of not readmitted is showing a low probability for both dots at the axis. This makes sense because an A1C result greater than 8 is considered a very high level and is serious for diabetic patients. Hence, the need to be Readmitted in combination of other factors. As you can see, this is a good way to see how a feature\'s value has impact on a model\'s prediction.')),(0,r.kt)("p",null,"This lab shows how the Feature Importance removes the black box way of not knowing how the model went about making a prediction. It\u2019s a global understanding of what key features the model uses to make a prediction. In addition, for each feature the user has the ability to explore and visualize how it is positively or negatively influencing the model\u2019s prediction for one class vs another for each value in the feature. This exposes the thresholds the model has to produce a certain outcome. We saw this in the ",(0,r.kt)("em",{parentName:"p"},"\u201cNumber_Diagnoses\u201d")," feature."))}p.isMDXComponent=!0},9149:(e,t,a)=>{a.d(t,{Z:()=>i});const i=a.p+"assets/images/10-fi-datapts-feature-contribution-e9245a158aec8a05d2c1bb71c8164548.png"},1355:(e,t,a)=>{a.d(t,{Z:()=>i});const i=a.p+"assets/images/10-fi-selected-datapt-influence-e61cd74c653326851f647a00a95bc6dd.png"},8979:(e,t,a)=>{a.d(t,{Z:()=>i});const i=a.p+"assets/images/10-fi-table-view-74587d988b5321538e4eed413af8b176.png"},4873:(e,t,a)=>{a.d(t,{Z:()=>i});const i=a.p+"assets/images/6-da-switch-cohort-bf7bd7b22e1618fed05807353b55873c.png"}}]);