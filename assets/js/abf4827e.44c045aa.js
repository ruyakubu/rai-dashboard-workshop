"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[27],{3905:(e,t,a)=>{a.d(t,{Zo:()=>p,kt:()=>d});var n=a(7294);function o(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function r(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function i(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?r(Object(a),!0).forEach((function(t){o(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):r(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function l(e,t){if(null==e)return{};var a,n,o=function(e,t){if(null==e)return{};var a,n,o={},r=Object.keys(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||(o[a]=e[a]);return o}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(o[a]=e[a])}return o}var s=n.createContext({}),c=function(e){var t=n.useContext(s),a=t;return e&&(a="function"==typeof e?e(t):i(i({},t),e)),a},p=function(e){var t=c(e.components);return n.createElement(s.Provider,{value:t},e.children)},u="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},f=n.forwardRef((function(e,t){var a=e.components,o=e.mdxType,r=e.originalType,s=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),u=c(a),f=o,d=u["".concat(s,".").concat(f)]||u[f]||m[f]||r;return a?n.createElement(d,i(i({ref:t},p),{},{components:a})):n.createElement(d,i({ref:t},p))}));function d(e,t){var a=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var r=a.length,i=new Array(r);i[0]=f;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[u]="string"==typeof e?e:o,i[1]=l;for(var c=2;c<r;c++)i[c]=a[c];return n.createElement.apply(null,i)}return n.createElement.apply(null,a)}f.displayName="MDXCreateElement"},2825:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>s,contentTitle:()=>i,default:()=>m,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var n=a(7462),o=(a(7294),a(3905));const r={id:"cs-analyze-image-lab4'",title:"Lab# 4: Analyze Image",sidebar_position:4,slug:"/content-safety-analyze-image"},i=void 0,l={unversionedId:"azure-content-safety/lab4-feature-importance/cs-analyze-image-lab4'",id:"azure-content-safety/lab4-feature-importance/cs-analyze-image-lab4'",title:"Lab# 4: Analyze Image",description:"There are tons of applications and social medial sites that enable users to upload images. This opens a flood gate of opportunities for users to upload sexual derogative content, violence, or harmful content. Similar to text, it\u2019s not realistic to rely on users to flag inappropriate content or the staff to manually see the content when it\u2019s uploaded.  Even with manual monitors, images are subjected to each individual evaluator to determine if it is risky.",source:"@site/docs/azure-content-safety/lab4-feature-importance/lab4-analyze-image.md",sourceDirName:"azure-content-safety/lab4-feature-importance",slug:"/content-safety-analyze-image",permalink:"/rai-dashboard-workshop/docs/content-safety-analyze-image",draft:!1,tags:[],version:"current",sidebarPosition:4,frontMatter:{id:"cs-analyze-image-lab4'",title:"Lab# 4: Analyze Image",sidebar_position:4,slug:"/content-safety-analyze-image"},sidebar:"mySidebar",previous:{title:"Lab# 3: Analyze Text",permalink:"/rai-dashboard-workshop/docs/content-safety-analyze-text"},next:{title:"Prompt Flow Overview",permalink:"/rai-dashboard-workshop/docs/prompt-flow-overview"}},s={},c=[],p={toc:c},u="wrapper";function m(e){let{components:t,...r}=e;return(0,o.kt)(u,(0,n.Z)({},p,r,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("p",null,(0,o.kt)("img",{src:a(5238).Z,width:"2061",height:"931"})),(0,o.kt)("p",null,"There are tons of applications and social medial sites that enable users to upload images. This opens a flood gate of opportunities for users to upload sexual derogative content, violence, or harmful content. Similar to text, it\u2019s not realistic to rely on users to flag inappropriate content or the staff to manually see the content when it\u2019s uploaded.  Even with manual monitors, images are subjected to each individual evaluator to determine if it is risky. "),(0,o.kt)("p",null,"In this lab, we will see examples of how one image is clearing inappropriately and another image can be subjective."),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},"Open the ",(0,o.kt)("em",{parentName:"li"},"image_content_safety.ipynb")," notebook."),(0,o.kt)("li",{parentName:"ol"},"Click ",(0,o.kt)("strong",{parentName:"li"},"Run All")," to execute the notebook."),(0,o.kt)("li",{parentName:"ol"},"Verify that all cells ran successfully."),(0,o.kt)("li",{parentName:"ol"},"In the project explorer, navigate to the \u201cimg\u201d folder and click on the ",(0,o.kt)("strong",{parentName:"li"},"image1.jpg")," to open it."),(0,o.kt)("li",{parentName:"ol"},"In the last cell, you\u2019ll see that the Content Safety API found the image of a gun and bullet to be \u201cViolence\u201d content with a severity of 2."),(0,o.kt)("li",{parentName:"ol"},"Now\u2026in cell#4, try changing image1.jpg to ",(0,o.kt)("strong",{parentName:"li"},"image2.jpg")," and see what the output will be.")),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Note"),": you can add code on how you want the application to behavior or what warning message to display, based on the severity level."))}m.isMDXComponent=!0},5238:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/cs-image-filter-1d236228e3d0507b7d93d3089aac228e.png"}}]);